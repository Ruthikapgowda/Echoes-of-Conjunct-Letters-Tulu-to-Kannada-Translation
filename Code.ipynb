{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc51c8b4-8013-4f20-9431-f6232b8f6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 112 sheets from folder: dataset/Lipi_1\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0001.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0002.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0003.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0004.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0005.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0006.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0007.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0008.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0009.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0010.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0011.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0012.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0013.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0014.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0015.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0016.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0017.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0018.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0019.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0020.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0021.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0022.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0023.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0024.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0025.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0026.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0027.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0028.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0029.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0030.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0031.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0032.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0033.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0034.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0035.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0036.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0037.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0038.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0039.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0040.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0041.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0042.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0043.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0044.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0045.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0046.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0047.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0048.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0049.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0050.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0051.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0052.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0053.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0054.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0055.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0056.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0057.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0058.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0059.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0060.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0061.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0062.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0063.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0064.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0065.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0066.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0067.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0068.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0069.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0070.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0071.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0072.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0073.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0074.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0075.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0076.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0077.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0078.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0079.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0080.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0081.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0082.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0083.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0084.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0085.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0086.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0087.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0088.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0089.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0090.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0091.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0092.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0093.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0094.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0095.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0096.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0097.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0098.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0099.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0100.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0101.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0102.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0103.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0104.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0105.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0106.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0107.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0108.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0109.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0110.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0111.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0112.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_1'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 0  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a0fb309-da78-4790-a4ff-33aab6ece9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_2\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 1_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 2_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 3_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 4_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 5_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 6_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 7_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 8_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 9_0001_page-0001.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_2'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 16  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c47833a4-bb41-4d08-aa7a-44c2d721bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_3\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 1_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 2_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 3_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 4_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 5_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 6_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 7_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 8_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 9_0001_page-0002.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_3'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 70  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c33ce50-4dfa-4f7a-a9a1-b043d1385cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_4\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 1_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 2_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 3_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 4_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 5_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 6_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 7_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 8_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 9_0001_page-0003.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_4'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 124  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2eafabb0-3a10-41a9-acb1-270000ee7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_5\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 1_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 2_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 3_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 4_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 5_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 6_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 7_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 8_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 9_0001_page-0004.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_5'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 178  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb0467d-49ba-4e2d-89a8-daaebcf340b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_6\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 1_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 2_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 3_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 4_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 5_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 6_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 7_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 8_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 9_0001_page-0005.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_6'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 232  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2bee1bb-f2b1-4bc9-82d9-8740922db37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_7\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 1_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 2_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 3_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 4_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 5_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 6_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 7_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 8_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 9_0001_page-0006.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_7'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 286  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe230904-9f9e-427c-b74e-eaa7f9e963bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_8\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 1_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 2_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 3_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 4_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 5_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 6_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 7_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 8_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 9_0001_page-0007.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_8'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 340  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79780e6d-73d7-4aa1-a062-196626752c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_9\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 1_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 2_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 3_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 4_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 5_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 6_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 7_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 8_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 9_0001_page-0008.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_9'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 394  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d4443ee-4898-4980-ac23-c7b17414974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_10\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 1_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 2_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 3_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 4_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 5_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 6_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 7_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 8_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 9_0001_page-0009.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_10'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 448  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c0fe0bd-c94f-4c44-b25e-c712c9da318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARIZATION\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_character_image(image, target_size=(50, 50)):\n",
    "    # Convert the image to grayscale if it isn't already\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    \n",
    "    # Apply adaptive thresholding (binarization) to get a binary image\n",
    "    binary_image = cv2.adaptiveThreshold(\n",
    "        blurred_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "        cv2.THRESH_BINARY_INV, 11, 8\n",
    "    )\n",
    "    \n",
    "    # Resize to the target size (50x50)\n",
    "    resized_image = cv2.resize(binary_image, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image = resized_image / 255.0\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def process_dataset(input_folder, output_folder, target_size=(64, 64)):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through each character folder\n",
    "    for character_folder in os.listdir(input_folder):\n",
    "        character_path = os.path.join(input_folder, character_folder)\n",
    "        \n",
    "        # Skip non-folder items\n",
    "        if not os.path.isdir(character_path):\n",
    "            continue\n",
    "\n",
    "        # Create the output directory for this character if it doesn’t exist\n",
    "        character_output_folder = os.path.join(output_folder, character_folder)\n",
    "        if not os.path.exists(character_output_folder):\n",
    "            os.makedirs(character_output_folder)\n",
    "        \n",
    "        # Loop through images in this character's folder\n",
    "        for img_file in os.listdir(character_path):\n",
    "            img_path = os.path.join(character_path, img_file)\n",
    "            \n",
    "            # Load the image\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is None:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Preprocess the image with binarization\n",
    "            preprocessed_image = preprocess_character_image(image, target_size)\n",
    "            \n",
    "            # Save the preprocessed image\n",
    "            output_image_path = os.path.join(character_output_folder, img_file)\n",
    "            # Multiply by 255 to convert back to uint8 for saving\n",
    "            cv2.imwrite(output_image_path, (preprocessed_image * 255).astype(np.uint8))\n",
    "\n",
    "input_folder = 'preprocessing'   # Input folder with original character images\n",
    "output_folder = 'binarization'  # Output folder for preprocessed images\n",
    "target_size = (50, 50)  # Target size for each character image\n",
    "\n",
    "# Process the dataset\n",
    "process_dataset(input_folder, output_folder, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7e3cc06-0a6c-4e1b-a8d4-4492d52cf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUGMENTATION\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "def augment_images_with_slant(input_folder, output_folder, target_size=(500, 500)):\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_subfolder = os.path.relpath(root, input_folder)\n",
    "\n",
    "            # Ensure the output subfolder exists\n",
    "            output_subfolder_path = os.path.join(output_folder, output_subfolder)\n",
    "            os.makedirs(output_subfolder_path, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                # Read the image\n",
    "                original_image = cv2.imread(input_path)\n",
    "\n",
    "                # Resize the image to the target size\n",
    "                resized_image = cv2.resize(original_image, target_size)\n",
    "\n",
    "                # Convert to grayscale and create a binary mask\n",
    "                gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "                _, binary_mask = cv2.threshold(gray_image, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # Invert the mask to get background as 255 and characters as 0\n",
    "                binary_mask = cv2.bitwise_not(binary_mask)\n",
    "\n",
    "                # Define the slant angles for left and right\n",
    "                left_slant_angle = 10  # degrees\n",
    "                right_slant_angle = -10  # degrees\n",
    "\n",
    "                # Center of the image\n",
    "                center = (target_size[1] // 2, target_size[0] // 2)\n",
    "\n",
    "                # Compute the rotation matrices\n",
    "                left_rotation_matrix = cv2.getRotationMatrix2D(center, left_slant_angle, 1)\n",
    "                right_rotation_matrix = cv2.getRotationMatrix2D(center, right_slant_angle, 1)\n",
    "\n",
    "                # Apply the slant (rotation) transformations\n",
    "                left_slanted_image = cv2.warpAffine(resized_image, left_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "                right_slanted_image = cv2.warpAffine(resized_image, right_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "                # Apply the same transformations to the binary mask\n",
    "                left_slanted_mask = cv2.warpAffine(binary_mask, left_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "                right_slanted_mask = cv2.warpAffine(binary_mask, right_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "\n",
    "                # Mask out the white areas introduced by the rotation\n",
    "                left_slanted_image[left_slanted_mask == 255] = (0, 0, 0)\n",
    "                right_slanted_image[right_slanted_mask == 255] = (0, 0, 0)\n",
    "\n",
    "                # Save the original and slanted images\n",
    "                original_output_path = os.path.join(output_subfolder_path, f\"original_{file}\")\n",
    "                left_slanted_output_path = os.path.join(output_subfolder_path, f\"left_slanted_{file}\")\n",
    "                right_slanted_output_path = os.path.join(output_subfolder_path, f\"right_slanted_{file}\")\n",
    "\n",
    "                cv2.imwrite(original_output_path, resized_image)\n",
    "                cv2.imwrite(left_slanted_output_path, left_slanted_image)\n",
    "                cv2.imwrite(right_slanted_output_path, right_slanted_image)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {input_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your input folder and output folder for images\n",
    "    input_folder = 'binarization' # Change this to your input images folder path\n",
    "    output_folder = 'augmentation' # Change this to your output folder for augmented images\n",
    "\n",
    "    # Augment the images with left and right slants, and save the results\n",
    "    augment_images_with_slant(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9436888e-2481-44a6-901b-c8567abc325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping saved to 'tulu_to_kannada_mapping.json'.\n"
     ]
    }
   ],
   "source": [
    "# MAPPING\n",
    "import os\n",
    "import json\n",
    "\n",
    "def map_tulu_to_kannada(root_folder, output_file=\"tulu_to_kannada_mapping.json\"):\n",
    "    # Kannada characters list (ensure it matches the dataset size)\n",
    "    kannada_characters = [\n",
    "     \"_\", \"ಅ\", \"ಆ\", \"ಇ\", \"ಈ\", \"ಉ\", \"ಊ\", \"ಋ\", \"ೠ\", \"ಎ\", \"ಏ\", \"ಐ\", \"ಒ\", \"ಔ\", \"ಅಂ\", \"ಅಃ\", \n",
    "    # ಕ family\n",
    "    \"ಕ\", \"ಕಾ\", \"ಕಿ\", \"ಕೀ\", \"ಕು\", \"ಕೂ\", \"ಕೃ\", \"ಕೆ\", \"ಕೈ\", \"ಕೊ\", \"ಕೌ\", \"ಕಂ\", \"ಕಃ\",\n",
    "    # ಖ family\n",
    "    \"ಖ\", \"ಖಾ\", \"ಖಿ\", \"ಖೀ\", \"ಖು\", \"ಖೂ\", \"ಖೃ\", \"ಖೆ\", \"ಖೈ\", \"ಖೊ\", \"ಖೌ\", \"ಖಂ\", \"ಖಃ\",\n",
    "    # ಗ family\n",
    "    \"ಗ\", \"ಗಾ\", \"ಗಿ\", \"ಗೀ\", \"ಗು\", \"ಗೂ\", \"ಗೃ\", \"ಗೆ\", \"ಗೈ\", \"ಗೊ\", \"ಗೌ\", \"ಗಂ\", \"ಗಃ\",\n",
    "    # ಘ family\n",
    "    \"ಘ\", \"ಘಾ\", \"ಘಿ\", \"ಘೀ\", \"ಘು\", \"ಘೂ\", \"ಘೃ\", \"ಘೆ\", \"ಘೈ\", \"ಘೊ\", \"ಘೌ\", \"ಘಂ\", \"ಘಃ\",\n",
    "    # ಙ family\n",
    "    \"ಙ\", \"ಙಾ\", \"ಙಿ\", \"ಙೀ\", \"ಙು\", \"ಙೂ\", \"ಙೃ\", \"ಙೆ\", \"ಙೈ\", \"ಙೊ\", \"ಙೌ\", \"ಙಂ\", \"ಙಃ\",\n",
    "    # ಚ family\n",
    "    \"ಚ\", \"ಚಾ\", \"ಚಿ\", \"ಚೀ\", \"ಚು\", \"ಚೂ\", \"ಚೃ\", \"ಚೆ\", \"ಚೈ\", \"ಚೊ\", \"ಚೌ\", \"ಚಂ\", \"ಚಃ\",\n",
    "    # ಛ family\n",
    "    \"ಛ\", \"ಛಾ\", \"ಛಿ\", \"ಛೀ\", \"ಛು\", \"ಛೂ\", \"ಛೃ\", \"ಛೆ\", \"ಛೈ\", \"ಛೊ\", \"ಛೌ\", \"ಛಂ\", \"ಛಃ\",\n",
    "    # ಜ family\n",
    "    \"ಜ\", \"ಜಾ\", \"ಜಿ\", \"ಜೀ\", \"ಜು\", \"ಜೂ\", \"ಜೃ\", \"ಜೆ\", \"ಜೈ\", \"ಜೊ\", \"ಜೌ\", \"ಜಂ\", \"ಜಃ\",\n",
    "    # ಝ family\n",
    "    \"ಝ\", \"ಝಾ\", \"ಝಿ\", \"ಝೀ\", \"ಝು\", \"ಝೂ\", \"ಝೃ\", \"ಝೆ\", \"ಝೈ\", \"ಝೊ\", \"ಝೌ\", \"ಝಂ\", \"ಝಃ\",\n",
    "    # ಞ family\n",
    "    \"ಞ\", \"ಞಾ\", \"ಞಿ\", \"ಞೀ\", \"ಞು\", \"ಞೂ\", \"ಞೃ\", \"ಞೆ\", \"ಞೈ\", \"ಞೊ\", \"ಞೌ\", \"ಞಂ\", \"ಞಃ\",\n",
    "    # ಟ family\n",
    "    \"ಟ\", \"ಟಾ\", \"ಟಿ\", \"ಟೀ\", \"ಟು\", \"ಟೂ\", \"ಟೃ\", \"ಟೆ\", \"ಟೈ\", \"ಟೊ\", \"ಟೌ\", \"ಟಂ\", \"ಟಃ\",\n",
    "    # ಠ family\n",
    "    \"ಠ\", \"ಠಾ\", \"ಠಿ\", \"ಠೀ\", \"ಠು\", \"ಠೂ\", \"ಠೃ\", \"ಠೆ\", \"ಠೈ\", \"ಠೊ\", \"ಠೌ\", \"ಠಂ\", \"ಠಃ\",\n",
    "    # ಡ family\n",
    "    \"ಡ\", \"ಡಾ\", \"ಡಿ\", \"ಡೀ\", \"ಡು\", \"ಡೂ\", \"ಡೃ\", \"ಡೆ\", \"ಡೈ\", \"ಡೊ\", \"ಡೌ\", \"ಡುಂ\", \"ಡಃ\",\n",
    "    # ಢ family\n",
    "    \"ಢ\", \"ಢಾ\", \"ಢಿ\", \"ಢೀ\", \"ಢು\", \"ಢೂ\", \"ಢೃ\", \"ಢೆ\", \"ಢೈ\", \"ಢೊ\", \"ಢೌ\", \"ಢಂ\", \"ಢಃ\",\n",
    "    # ಣ family\n",
    "    \"ಣ\", \"ಣಾ\", \"ಣಿ\", \"ಣೀ\", \"ಣು\", \"ಣೂ\", \"ಣೃ\", \"ಣೆ\", \"ಣೈ\", \"ಣೊ\", \"ಣೌ\", \"ಣಂ\", \"ಣಃ\",\n",
    "    # ತ family\n",
    "    \"ತ\", \"ತಾ\", \"ತಿ\", \"ತೀ\", \"ತು\", \"ತೂ\", \"ತೃ\", \"ತೆ\", \"ತೈ\", \"ತೊ\", \"ತೌ\", \"ತಂ\", \"ತಃ\",\n",
    "    # ಥ family\n",
    "    \"ಥ\", \"ಥಾ\", \"ಥಿ\", \"ಥೀ\", \"ಥು\", \"ಥೂ\", \"ಥೃ\", \"ಥೆ\", \"ಥೈ\", \"ಥೊ\", \"ಥೌ\", \"ಥಂ\", \"ಥಃ\",\n",
    "    # ದ family\n",
    "    \"ದ\", \"ದಾ\", \"ದಿ\", \"ದೀ\", \"ದು\", \"ದೂ\", \"ದೃ\", \"ದೆ\", \"ದೈ\", \"ದೊ\", \"ದೌ\", \"ದಂ\", \"ದಃ\",\n",
    "    # ಧ family\n",
    "    \"ಧ\", \"ಧಾ\", \"ಧಿ\", \"ಧೀ\", \"ಧು\", \"ಧೂ\", \"ಧೃ\", \"ಧೆ\", \"ಧೈ\", \"ಧೊ\", \"ಧೌ\", \"ಧಂ\", \"ಧಃ\",\n",
    "    # ನ family\n",
    "    \"ನ\", \"ನಾ\", \"ನಿ\", \"ನೀ\", \"ನು\", \"ನೂ\", \"ನೃ\", \"ನೆ\", \"ನೈ\", \"ನೊ\", \"ನೌ\", \"ನಂ\", \"ನಃ\",\n",
    "    # ಪ family\n",
    "    \"ಪ\", \"ಪಾ\", \"ಪಿ\", \"ಪೀ\", \"ಪು\", \"ಪೂ\", \"ಪೃ\", \"ಪೆ\", \"ಪೈ\", \"ಪೊ\", \"ಪೌ\", \"ಪಂ\", \"ಪಃ\",\n",
    "    # ಫ family\n",
    "    \"ಫ\", \"ಫಾ\", \"ಫಿ\", \"ಫೀ\", \"ಫು\", \"ಫೂ\", \"ಫೃ\", \"ಫೆ\", \"ಫೈ\", \"ಫೊ\", \"ಫೌ\", \"ಫಂ\", \"ಫಃ\",\n",
    "    # ಬ family\n",
    "    \"ಬ\", \"ಬಾ\", \"ಬಿ\", \"ಬೀ\", \"ಬು\", \"ಬೂ\", \"ಬೃ\", \"ಬೆ\", \"ಬೈ\", \"ಬೊ\", \"ಬೌ\", \"ಬಂ\", \"ಬಃ\",\n",
    "    # ಭ family\n",
    "    \"ಭ\", \"ಭಾ\", \"ಭಿ\", \"ಭೀ\", \"ಭು\", \"ಭೂ\", \"ಭೃ\", \"ಭೆ\", \"ಭೈ\", \"ಭೊ\", \"ಭೌ\", \"ಭಂ\", \"ಭಃ\",\n",
    "    # ಮ family\n",
    "    \"ಮ\", \"ಮಾ\", \"ಮಿ\", \"ಮೀ\", \"ಮು\", \"ಮೂ\", \"ಮೃ\", \"ಮೆ\", \"ಮೈ\", \"ಮೊ\", \"ಮೌ\", \"ಮಂ\", \"ಮಃ\",\n",
    "    # ಯ family\n",
    "    \"ಯ\", \"ಯಾ\", \"ಯಿ\", \"ಯೀ\", \"ಯು\", \"ಯೂ\", \"ಯೃ\", \"ಯೆ\", \"ಯೈ\", \"ಯೊ\", \"ಯೌ\", \"ಯಂ\", \"ಯಃ\",\n",
    "    # ರ family\n",
    "    \"ರ\", \"ರಾ\", \"ರಿ\", \"ರೀ\", \"ರು\", \"ರೂ\", \"ರೃ\", \"ರೆ\", \"ರೈ\", \"ರೊ\", \"ರೌ\", \"ರಂ\", \"ರಃ\",\n",
    "    # ಲ family\n",
    "    \"ಲ\", \"ಲಾ\", \"ಲಿ\", \"ಲೀ\", \"ಲು\", \"ಲೂ\", \"ಲೃ\", \"ಲೆ\", \"ಲೈ\", \"ಲೊ\", \"ಲೌ\", \"ಲಂ\", \"ಲಃ\",\n",
    "    # ವ family\n",
    "    \"ವ\", \"ವಾ\", \"ವಿ\", \"ವೀ\", \"ವು\", \"ವೂ\", \"ವೃ\", \"ವೆ\", \"ವೈ\", \"ವೊ\", \"ವೌ\", \"ವಂ\", \"ವಃ\",\n",
    "    # ಶ family \n",
    "    \"ಶ\", \"ಶಾ\", \"ಶಿ\", \"ಶೀ\", \"ಶು\", \"ಶೂ\", \"ಶೃ\", \"ಶೆ\", \"ಶೈ\", \"ಶೊ\", \"ಶೌ\", \"ಶಂ\", \"ಶಃ\",\n",
    "    # ಷ family \n",
    "    \"ಷ\", \"ಷಾ\", \"ಷಿ\", \"ಷೀ\", \"ಷು\", \"ಷೂ\", \"ಷೃ\", \"ಷೆ\", \"ಷೈ\", \"ಷೊ\", \"ಷೌ\", \"ಷಂ\", \"ಷಃ\",\n",
    "    # ಸ family \n",
    "    \"ಸ\", \"ಸಾ\", \"ಸಿ\", \"ಸೀ\", \"ಸು\", \"ಸೂ\", \"ಸೃ\", \"ಸೆ\", \"ಸೈ\", \"ಸೊ\", \"ಸೌ\", \"ಸಂ\", \"ಸಃ\",\n",
    "    # ಹ family\n",
    "    \"ಹ\", \"ಹಾ\", \"ಹಿ\", \"ಹೀ\", \"ಹು\", \"ಹೂ\", \"ಹೃ\", \"ಹೆ\", \"ಹೈ\", \"ಹೊ\", \"ಹೌ\", \"ಹಂ\", \"ಹಃ\",\n",
    "    # ಳ family\n",
    "    \"ಳ\", \"ಳಾ\", \"ಳಿ\", \"ಳೀ\", \"ಳು\", \"ಳೂ\", \"ಳೃ\", \"ಳೆ\", \"ಳೈ\", \"ಳೊ\", \"ಳೌ\", \"ಳಂ\", \"ಳಃ\"\n",
    "    ]  \n",
    "\n",
    "    # Get all folder names in the root directory\n",
    "    folder_names = [f for f in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, f))]\n",
    "\n",
    "    # Ensure correct ordering (sort numerically if folder names contain numbers)\n",
    "    folder_names_sorted = sorted(folder_names, key=lambda x: int(''.join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else x)\n",
    "\n",
    "    # Check if we have enough Kannada characters\n",
    "    if len(folder_names_sorted) > len(kannada_characters):\n",
    "        print(\"Error: Not enough Kannada characters available for mapping.\")\n",
    "        return\n",
    "\n",
    "    # Create a dictionary mapping each folder to a Kannada character\n",
    "    mapping = {folder: kannada_characters[idx] for idx, folder in enumerate(folder_names_sorted)}\n",
    "\n",
    "    # Save mapping to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(mapping, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Mapping saved to '{output_file}'.\")\n",
    "    return mapping\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = 'augmentation'  # Update with your dataset path\n",
    "    mapping = map_tulu_to_kannada(root_folder)\n",
    "    # print(mapping,) Print to verify mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d53f488f-b555-4a1d-a1f4-02459a7d77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████████████████████████████████████████████████████████████| 458/458 [03:36<00:00,  2.11it/s]\n",
      "Extracting Features: 100%|██████████████████████████████████████████████████████| 17310/17310 [00:30<00:00, 576.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM Model...\n",
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.7158\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.81      0.84      0.83        76\n",
      "           ಅ       0.62      0.85      0.72        66\n",
      "          ಅಂ       0.71      0.66      0.68        80\n",
      "          ಅಃ       0.58      0.73      0.65        63\n",
      "           ಆ       0.83      0.90      0.86        58\n",
      "           ಇ       0.69      0.84      0.76        69\n",
      "           ಈ       0.73      0.81      0.77        73\n",
      "           ಉ       0.66      0.81      0.72        52\n",
      "           ಊ       0.78      0.82      0.80        71\n",
      "           ಋ       0.87      0.92      0.90        66\n",
      "           ಎ       0.81      0.85      0.83        66\n",
      "           ಏ       0.81      0.88      0.84        73\n",
      "           ಐ       0.84      0.85      0.85        62\n",
      "           ಒ       0.86      0.85      0.86        73\n",
      "           ಔ       0.79      0.77      0.78        62\n",
      "           ಕ       0.71      0.83      0.77         6\n",
      "          ಕಂ       1.00      1.00      1.00         2\n",
      "          ಕಃ       1.00      0.40      0.57         5\n",
      "          ಕಾ       0.83      1.00      0.91         5\n",
      "          ಕಿ       0.62      0.83      0.71         6\n",
      "          ಕೀ       1.00      0.70      0.82        10\n",
      "          ಕು       0.44      0.80      0.57         5\n",
      "          ಕೂ       0.80      0.57      0.67         7\n",
      "          ಕೃ       0.00      0.00      0.00         2\n",
      "          ಕೆ       0.56      1.00      0.71         5\n",
      "          ಕೈ       0.00      0.00      0.00         5\n",
      "          ಕೊ       0.75      0.75      0.75         4\n",
      "          ಕೌ       0.00      0.00      0.00         0\n",
      "           ಖ       0.80      0.80      0.80         5\n",
      "          ಖಂ       1.00      0.75      0.86         4\n",
      "          ಖಃ       0.90      0.90      0.90        10\n",
      "          ಖಾ       0.67      0.67      0.67         6\n",
      "          ಖಿ       0.43      0.60      0.50         5\n",
      "          ಖೀ       1.00      0.80      0.89         5\n",
      "          ಖು       0.00      0.00      0.00         5\n",
      "          ಖೂ       0.71      0.83      0.77         6\n",
      "          ಖೃ       1.00      0.60      0.75         5\n",
      "          ಖೆ       1.00      0.56      0.71         9\n",
      "          ಖೈ       0.50      0.40      0.44         5\n",
      "          ಖೊ       0.50      0.67      0.57         3\n",
      "          ಖೌ       1.00      0.50      0.67         6\n",
      "           ಗ       0.80      0.67      0.73         6\n",
      "          ಗಂ       1.00      1.00      1.00         9\n",
      "          ಗಃ       0.83      1.00      0.91         5\n",
      "          ಗಾ       0.43      0.60      0.50         5\n",
      "          ಗಿ       1.00      0.62      0.77         8\n",
      "          ಗೀ       0.67      0.80      0.73         5\n",
      "          ಗು       0.67      0.22      0.33         9\n",
      "          ಗೂ       0.67      0.57      0.62         7\n",
      "          ಗೃ       0.29      0.50      0.36         4\n",
      "          ಗೆ       0.43      0.60      0.50         5\n",
      "          ಗೈ       0.50      0.50      0.50         4\n",
      "          ಗೊ       0.57      0.67      0.62         6\n",
      "          ಗೌ       0.57      1.00      0.73         4\n",
      "           ಘ       0.80      0.57      0.67         7\n",
      "          ಘಂ       0.25      0.33      0.29         3\n",
      "          ಘಃ       1.00      0.44      0.62         9\n",
      "          ಘಾ       0.60      0.75      0.67         4\n",
      "          ಘಿ       0.40      0.29      0.33         7\n",
      "          ಘೀ       0.33      0.67      0.44         3\n",
      "          ಘು       1.00      0.80      0.89         5\n",
      "          ಘೂ       0.67      1.00      0.80         2\n",
      "          ಘೃ       0.67      1.00      0.80         4\n",
      "          ಘೆ       0.75      0.75      0.75         4\n",
      "          ಘೈ       0.62      1.00      0.77         5\n",
      "          ಘೊ       1.00      0.20      0.33         5\n",
      "          ಘೌ       0.40      1.00      0.57         2\n",
      "           ಙ       1.00      1.00      1.00         5\n",
      "          ಙಂ       0.83      0.83      0.83         6\n",
      "          ಙಃ       1.00      1.00      1.00         4\n",
      "          ಙಾ       0.62      0.83      0.71         6\n",
      "          ಙಿ       0.57      1.00      0.73         4\n",
      "          ಙೀ       0.60      0.43      0.50         7\n",
      "          ಙು       0.60      0.50      0.55         6\n",
      "          ಙೂ       0.75      0.67      0.71         9\n",
      "          ಙೃ       1.00      0.50      0.67         2\n",
      "          ಙೆ       0.33      0.14      0.20         7\n",
      "          ಙೈ       0.62      0.71      0.67         7\n",
      "          ಙೊ       1.00      0.67      0.80         6\n",
      "          ಙೌ       1.00      1.00      1.00         4\n",
      "           ಚ       0.60      1.00      0.75         6\n",
      "          ಚಂ       0.67      0.80      0.73         5\n",
      "          ಚಃ       0.57      0.80      0.67         5\n",
      "          ಚಾ       0.50      0.67      0.57         3\n",
      "          ಚಿ       0.50      0.75      0.60         4\n",
      "          ಚೀ       0.40      0.40      0.40         5\n",
      "          ಚು       0.38      0.50      0.43         6\n",
      "          ಚೂ       0.57      0.40      0.47        10\n",
      "          ಚೃ       0.80      0.67      0.73         6\n",
      "          ಚೆ       0.80      0.67      0.73         6\n",
      "          ಚೈ       0.43      0.75      0.55         4\n",
      "          ಚೊ       0.00      0.00      0.00         3\n",
      "          ಚೌ       0.20      0.17      0.18         6\n",
      "           ಛ       0.60      0.75      0.67         4\n",
      "          ಛಂ       0.67      0.67      0.67         3\n",
      "          ಛಃ       0.75      0.43      0.55         7\n",
      "          ಛಾ       0.71      0.83      0.77         6\n",
      "          ಛಿ       0.67      0.67      0.67         3\n",
      "          ಛೀ       1.00      0.38      0.55         8\n",
      "          ಛು       0.50      0.67      0.57         6\n",
      "          ಛೂ       1.00      0.43      0.60         7\n",
      "          ಛೃ       0.57      0.67      0.62         6\n",
      "          ಛೆ       0.43      0.43      0.43         7\n",
      "          ಛೈ       0.71      0.50      0.59        10\n",
      "          ಛೊ       1.00      0.33      0.50         9\n",
      "          ಛೌ       0.56      1.00      0.71         5\n",
      "           ಜ       0.25      0.25      0.25         4\n",
      "          ಜಂ       0.33      0.33      0.33         3\n",
      "          ಜಃ       0.57      0.80      0.67         5\n",
      "          ಜಾ       0.57      1.00      0.73         4\n",
      "          ಜಿ       0.86      1.00      0.92         6\n",
      "          ಜೀ       1.00      0.75      0.86         8\n",
      "          ಜು       0.00      0.00      0.00         2\n",
      "          ಜೂ       0.40      0.40      0.40         5\n",
      "          ಜೃ       0.40      0.40      0.40         5\n",
      "          ಜೆ       0.67      0.67      0.67         6\n",
      "          ಜೈ       0.56      0.83      0.67         6\n",
      "          ಜೊ       1.00      0.50      0.67         6\n",
      "          ಜೌ       0.60      1.00      0.75         3\n",
      "           ಝ       0.67      0.67      0.67         6\n",
      "          ಝಂ       1.00      0.62      0.77         8\n",
      "          ಝಃ       0.25      0.67      0.36         3\n",
      "          ಝಾ       0.44      1.00      0.62         4\n",
      "          ಝಿ       0.57      0.57      0.57         7\n",
      "          ಝೀ       0.67      0.40      0.50         5\n",
      "          ಝು       0.83      0.71      0.77         7\n",
      "          ಝೂ       0.50      1.00      0.67         3\n",
      "          ಝೃ       1.00      0.29      0.44         7\n",
      "          ಝೆ       0.88      0.88      0.88         8\n",
      "          ಝೈ       0.88      0.70      0.78        10\n",
      "          ಝೊ       0.80      1.00      0.89         4\n",
      "          ಝೌ       0.00      0.00      0.00         3\n",
      "           ಞ       0.00      0.00      0.00         2\n",
      "          ಞಂ       0.67      0.50      0.57         4\n",
      "          ಞಃ       0.67      0.67      0.67         3\n",
      "          ಞಾ       1.00      0.33      0.50         3\n",
      "          ಞಿ       0.60      0.60      0.60         5\n",
      "          ಞೀ       0.56      1.00      0.71         5\n",
      "          ಞು       0.86      0.60      0.71        10\n",
      "          ಞೂ       0.33      0.17      0.22         6\n",
      "          ಞೃ       0.50      0.75      0.60         4\n",
      "          ಞೆ       1.00      0.67      0.80         6\n",
      "          ಞೈ       0.42      0.83      0.56         6\n",
      "          ಞೊ       0.67      0.67      0.67         3\n",
      "          ಞೌ       1.00      0.86      0.92         7\n",
      "           ಟ       0.90      0.90      0.90        10\n",
      "          ಟಂ       0.75      0.60      0.67         5\n",
      "          ಟಃ       0.50      1.00      0.67         3\n",
      "          ಟಾ       1.00      0.50      0.67         6\n",
      "          ಟಿ       0.44      0.80      0.57         5\n",
      "          ಟೀ       0.75      0.38      0.50         8\n",
      "          ಟು       1.00      1.00      1.00         3\n",
      "          ಟೂ       0.80      1.00      0.89         4\n",
      "          ಟೃ       1.00      0.64      0.78        11\n",
      "          ಟೆ       1.00      1.00      1.00         2\n",
      "          ಟೈ       1.00      1.00      1.00         3\n",
      "          ಟೊ       0.50      1.00      0.67         2\n",
      "          ಟೌ       0.83      1.00      0.91         5\n",
      "           ಠ       0.77      0.77      0.77        13\n",
      "          ಠಂ       1.00      1.00      1.00         6\n",
      "          ಠಃ       0.60      1.00      0.75         6\n",
      "          ಠಾ       0.75      1.00      0.86         6\n",
      "          ಠಿ       0.86      0.75      0.80         8\n",
      "          ಠೀ       1.00      0.83      0.91         6\n",
      "          ಠು       0.43      0.60      0.50         5\n",
      "          ಠೂ       1.00      0.56      0.71         9\n",
      "          ಠೃ       0.90      0.82      0.86        11\n",
      "          ಠೆ       0.83      0.71      0.77         7\n",
      "          ಠೈ       0.78      1.00      0.88         7\n",
      "          ಠೊ       0.86      0.86      0.86         7\n",
      "          ಠೌ       0.88      1.00      0.93         7\n",
      "           ಡ       0.33      0.17      0.22         6\n",
      "          ಡಃ       0.60      0.43      0.50         7\n",
      "          ಡಾ       0.75      0.75      0.75         4\n",
      "          ಡಿ       0.67      0.67      0.67         3\n",
      "          ಡೀ       0.75      0.38      0.50         8\n",
      "          ಡು       0.57      0.80      0.67         5\n",
      "         ಡುಂ       0.50      0.83      0.62         6\n",
      "          ಡೂ       0.80      0.67      0.73         6\n",
      "          ಡೃ       0.20      0.14      0.17         7\n",
      "          ಡೆ       0.50      0.43      0.46         7\n",
      "          ಡೈ       0.00      0.00      0.00         4\n",
      "          ಡೊ       0.60      0.23      0.33        13\n",
      "          ಡೌ       0.29      0.67      0.40         3\n",
      "           ಢ       0.62      0.83      0.71         6\n",
      "          ಢಂ       1.00      0.50      0.67         6\n",
      "          ಢಃ       0.57      0.67      0.62         6\n",
      "          ಢಾ       0.60      0.38      0.46         8\n",
      "          ಢಿ       0.75      0.50      0.60         6\n",
      "          ಢೀ       0.57      0.67      0.62         6\n",
      "          ಢು       0.75      1.00      0.86         3\n",
      "          ಢೂ       0.33      0.75      0.46         4\n",
      "          ಢೃ       0.50      0.43      0.46         7\n",
      "          ಢೆ       0.60      0.50      0.55         6\n",
      "          ಢೈ       0.71      0.83      0.77         6\n",
      "          ಢೊ       0.57      0.57      0.57         7\n",
      "          ಢೌ       0.75      0.86      0.80         7\n",
      "           ಣ       0.71      0.83      0.77         6\n",
      "          ಣಂ       0.50      0.67      0.57         3\n",
      "          ಣಃ       0.67      0.67      0.67         3\n",
      "          ಣಾ       1.00      0.25      0.40         4\n",
      "          ಣಿ       0.57      1.00      0.73         4\n",
      "          ಣೀ       0.80      0.67      0.73         6\n",
      "          ಣು       0.67      0.80      0.73         5\n",
      "          ಣೂ       0.50      0.20      0.29         5\n",
      "          ಣೃ       1.00      0.40      0.57         5\n",
      "          ಣೆ       0.80      0.57      0.67         7\n",
      "          ಣೈ       0.50      0.60      0.55         5\n",
      "          ಣೊ       0.71      1.00      0.83         5\n",
      "          ಣೌ       0.75      1.00      0.86         3\n",
      "           ತ       1.00      0.67      0.80         3\n",
      "          ತಂ       1.00      0.75      0.86         4\n",
      "          ತಃ       1.00      1.00      1.00         4\n",
      "          ತಾ       1.00      1.00      1.00         5\n",
      "          ತಿ       1.00      1.00      1.00         5\n",
      "          ತೀ       1.00      0.75      0.86         4\n",
      "          ತು       0.60      0.60      0.60         5\n",
      "          ತೂ       1.00      0.86      0.92         7\n",
      "          ತೃ       0.33      0.33      0.33         3\n",
      "          ತೆ       0.67      1.00      0.80         4\n",
      "          ತೈ       0.88      0.88      0.88         8\n",
      "          ತೊ       1.00      0.75      0.86         4\n",
      "          ತೌ       1.00      0.86      0.92         7\n",
      "           ಥ       1.00      0.71      0.83         7\n",
      "          ಥಂ       0.67      1.00      0.80         2\n",
      "          ಥಃ       1.00      0.80      0.89         5\n",
      "          ಥಾ       0.33      0.50      0.40         4\n",
      "          ಥಿ       0.86      0.75      0.80         8\n",
      "          ಥೀ       0.80      0.44      0.57         9\n",
      "          ಥು       0.83      1.00      0.91         5\n",
      "          ಥೂ       1.00      1.00      1.00         4\n",
      "          ಥೃ       0.75      1.00      0.86         3\n",
      "          ಥೆ       0.50      0.57      0.53         7\n",
      "          ಥೈ       1.00      0.40      0.57         5\n",
      "          ಥೊ       0.25      0.25      0.25         4\n",
      "          ಥೌ       0.29      0.67      0.40         3\n",
      "           ದ       0.25      0.20      0.22         5\n",
      "          ದಂ       0.80      0.80      0.80         5\n",
      "          ದಃ       1.00      0.57      0.73         7\n",
      "          ದಾ       0.75      0.60      0.67         5\n",
      "          ದಿ       0.71      1.00      0.83         5\n",
      "          ದೀ       1.00      0.86      0.92         7\n",
      "          ದು       1.00      0.50      0.67         6\n",
      "          ದೂ       0.71      1.00      0.83         5\n",
      "          ದೃ       1.00      0.80      0.89         5\n",
      "          ದೆ       1.00      1.00      1.00         3\n",
      "          ದೈ       0.57      0.80      0.67         5\n",
      "          ದೊ       0.83      0.42      0.56        12\n",
      "          ದೌ       0.67      0.57      0.62         7\n",
      "           ಧ       1.00      1.00      1.00         3\n",
      "          ಧಂ       1.00      0.88      0.93         8\n",
      "          ಧಃ       1.00      0.67      0.80         6\n",
      "          ಧಾ       0.83      0.83      0.83         6\n",
      "          ಧಿ       0.00      0.00      0.00         2\n",
      "          ಧೀ       0.62      1.00      0.77         5\n",
      "          ಧು       1.00      0.71      0.83         7\n",
      "          ಧೂ       0.38      0.75      0.50         4\n",
      "          ಧೃ       1.00      0.33      0.50         6\n",
      "          ಧೆ       0.60      0.75      0.67         4\n",
      "          ಧೈ       0.45      1.00      0.62         5\n",
      "          ಧೊ       1.00      0.67      0.80         9\n",
      "          ಧೌ       0.50      1.00      0.67         2\n",
      "           ನ       0.25      0.33      0.29         3\n",
      "          ನಂ       1.00      0.86      0.92         7\n",
      "          ನಃ       1.00      1.00      1.00         5\n",
      "          ನಾ       0.75      0.75      0.75         4\n",
      "          ನಿ       1.00      1.00      1.00         6\n",
      "          ನೀ       0.80      1.00      0.89         4\n",
      "          ನು       0.75      0.75      0.75         4\n",
      "          ನೂ       0.50      1.00      0.67         1\n",
      "          ನೃ       0.83      0.83      0.83         6\n",
      "          ನೆ       0.00      0.00      0.00         0\n",
      "          ನೈ       0.56      0.71      0.62         7\n",
      "          ನೊ       0.50      0.20      0.29         5\n",
      "          ನೌ       1.00      0.80      0.89         5\n",
      "           ಪ       1.00      0.80      0.89         5\n",
      "          ಪಂ       1.00      1.00      1.00         5\n",
      "          ಪಃ       0.00      0.00      0.00         2\n",
      "          ಪಾ       0.67      0.50      0.57         4\n",
      "          ಪಿ       0.40      0.67      0.50         3\n",
      "          ಪೀ       0.83      0.56      0.67         9\n",
      "          ಪು       0.57      1.00      0.73         4\n",
      "          ಪೂ       1.00      0.62      0.77         8\n",
      "          ಪೃ       1.00      0.50      0.67         6\n",
      "          ಪೆ       1.00      0.71      0.83         7\n",
      "          ಪೈ       0.50      0.40      0.44         5\n",
      "          ಪೊ       0.86      0.86      0.86         7\n",
      "          ಪೌ       0.50      0.50      0.50         4\n",
      "           ಫ       0.67      0.50      0.57         4\n",
      "          ಫಂ       0.83      1.00      0.91         5\n",
      "          ಫಃ       1.00      0.60      0.75         5\n",
      "          ಫಾ       0.83      0.83      0.83         6\n",
      "          ಫಿ       0.71      0.83      0.77         6\n",
      "          ಫೀ       0.50      0.11      0.18         9\n",
      "          ಫು       0.60      0.60      0.60         5\n",
      "          ಫೂ       0.33      0.50      0.40         4\n",
      "          ಫೃ       0.40      0.50      0.44         4\n",
      "          ಫೆ       0.75      0.75      0.75         4\n",
      "          ಫೈ       0.67      0.57      0.62         7\n",
      "          ಫೊ       0.43      0.60      0.50         5\n",
      "          ಫೌ       0.33      0.25      0.29         4\n",
      "           ಬ       0.67      0.80      0.73         5\n",
      "          ಬಂ       1.00      1.00      1.00         3\n",
      "          ಬಃ       0.60      0.75      0.67         4\n",
      "          ಬಾ       0.00      0.00      0.00         4\n",
      "          ಬಿ       0.50      0.60      0.55         5\n",
      "          ಬೀ       0.75      0.43      0.55         7\n",
      "          ಬು       0.57      0.67      0.62         6\n",
      "          ಬೂ       0.50      0.29      0.36         7\n",
      "          ಬೃ       0.67      0.50      0.57         4\n",
      "          ಬೆ       0.50      0.50      0.50         2\n",
      "          ಬೈ       0.83      0.71      0.77         7\n",
      "          ಬೊ       0.75      0.75      0.75         4\n",
      "          ಬೌ       1.00      0.50      0.67         8\n",
      "           ಭ       0.50      0.80      0.62         5\n",
      "          ಭಂ       1.00      1.00      1.00         5\n",
      "          ಭಃ       1.00      0.50      0.67         6\n",
      "          ಭಾ       0.18      0.67      0.29         3\n",
      "          ಭಿ       0.67      1.00      0.80         6\n",
      "          ಭೀ       1.00      0.50      0.67         8\n",
      "          ಭು       0.50      0.25      0.33         4\n",
      "          ಭೂ       0.50      0.50      0.50         6\n",
      "          ಭೃ       1.00      1.00      1.00         3\n",
      "          ಭೆ       0.75      0.60      0.67         5\n",
      "          ಭೈ       1.00      0.71      0.83         7\n",
      "          ಭೊ       1.00      0.33      0.50         6\n",
      "          ಭೌ       0.50      1.00      0.67         4\n",
      "           ಮ       0.50      0.80      0.62         5\n",
      "          ಮಂ       0.86      1.00      0.92         6\n",
      "          ಮಃ       1.00      1.00      1.00         1\n",
      "          ಮಾ       0.80      0.67      0.73         6\n",
      "          ಮಿ       1.00      0.60      0.75         5\n",
      "          ಮೀ       1.00      1.00      1.00         8\n",
      "          ಮು       0.67      0.57      0.62         7\n",
      "          ಮೂ       0.50      0.60      0.55         5\n",
      "          ಮೃ       0.60      0.50      0.55         6\n",
      "          ಮೆ       1.00      0.83      0.91         6\n",
      "          ಮೈ       0.75      1.00      0.86         3\n",
      "          ಮೊ       0.50      1.00      0.67         3\n",
      "          ಮೌ       1.00      1.00      1.00         5\n",
      "           ಯ       0.67      0.25      0.36         8\n",
      "          ಯಂ       0.83      0.71      0.77         7\n",
      "          ಯಃ       1.00      0.86      0.92         7\n",
      "          ಯಾ       0.50      0.25      0.33         4\n",
      "          ಯಿ       0.67      0.67      0.67         6\n",
      "          ಯೀ       0.83      0.71      0.77         7\n",
      "          ಯು       0.57      0.57      0.57         7\n",
      "          ಯೂ       0.20      0.25      0.22         4\n",
      "          ಯೃ       0.75      0.60      0.67         5\n",
      "          ಯೆ       0.67      0.44      0.53         9\n",
      "          ಯೈ       0.43      1.00      0.60         3\n",
      "          ಯೊ       0.40      0.40      0.40         5\n",
      "          ಯೌ       0.60      0.50      0.55         6\n",
      "           ರ       0.71      1.00      0.83         5\n",
      "          ರಂ       1.00      0.88      0.93         8\n",
      "          ರಃ       1.00      1.00      1.00         6\n",
      "          ರಾ       1.00      0.80      0.89         5\n",
      "          ರಿ       0.86      1.00      0.92         6\n",
      "          ರೀ       1.00      1.00      1.00         3\n",
      "          ರು       1.00      1.00      1.00         3\n",
      "          ರೂ       1.00      1.00      1.00         5\n",
      "          ರೃ       1.00      1.00      1.00         5\n",
      "          ರೆ       1.00      0.36      0.53        11\n",
      "          ರೈ       0.60      1.00      0.75         6\n",
      "          ರೊ       0.80      0.80      0.80         5\n",
      "          ರೌ       0.60      0.60      0.60         5\n",
      "           ಲ       1.00      0.50      0.67         8\n",
      "          ಲಂ       0.86      0.86      0.86         7\n",
      "          ಲಃ       0.60      0.75      0.67         8\n",
      "          ಲಾ       0.67      1.00      0.80         4\n",
      "          ಲಿ       0.67      1.00      0.80         2\n",
      "          ಲೀ       1.00      1.00      1.00         5\n",
      "          ಲು       0.67      1.00      0.80         4\n",
      "          ಲೂ       1.00      0.40      0.57         5\n",
      "          ಲೃ       1.00      0.60      0.75         5\n",
      "          ಲೆ       0.57      0.50      0.53         8\n",
      "          ಲೈ       1.00      1.00      1.00         7\n",
      "          ಲೊ       0.67      1.00      0.80         4\n",
      "          ಲೌ       0.60      1.00      0.75         3\n",
      "           ಳ       0.25      0.17      0.20         6\n",
      "          ಳಂ       1.00      0.50      0.67        10\n",
      "          ಳಃ       0.75      0.75      0.75         4\n",
      "          ಳಾ       0.89      0.80      0.84        10\n",
      "          ಳಿ       0.80      1.00      0.89         4\n",
      "          ಳೀ       1.00      0.67      0.80         3\n",
      "          ಳು       0.75      0.50      0.60         6\n",
      "          ಳೂ       0.62      0.83      0.71         6\n",
      "          ಳೃ       0.80      1.00      0.89         4\n",
      "          ಳೆ       0.80      0.80      0.80         5\n",
      "          ಳೈ       1.00      0.60      0.75         5\n",
      "          ಳೊ       0.60      1.00      0.75         3\n",
      "          ಳೌ       1.00      0.78      0.88         9\n",
      "           ವ       0.91      1.00      0.95        10\n",
      "          ವಂ       0.75      1.00      0.86         3\n",
      "          ವಃ       0.83      0.83      0.83         6\n",
      "          ವಾ       0.20      0.50      0.29         4\n",
      "          ವಿ       0.71      0.56      0.62         9\n",
      "          ವೀ       1.00      0.33      0.50         6\n",
      "          ವು       0.33      1.00      0.50         1\n",
      "          ವೂ       1.00      0.71      0.83         7\n",
      "          ವೃ       1.00      0.75      0.86         8\n",
      "          ವೆ       0.67      0.50      0.57         8\n",
      "          ವೈ       0.67      0.29      0.40         7\n",
      "          ವೊ       0.67      0.80      0.73         5\n",
      "          ವೌ       0.75      0.75      0.75         4\n",
      "           ಶ       0.80      1.00      0.89         4\n",
      "          ಶಂ       1.00      1.00      1.00         6\n",
      "          ಶಃ       0.67      1.00      0.80         2\n",
      "          ಶಾ       0.50      1.00      0.67         1\n",
      "          ಶಿ       1.00      0.57      0.73         7\n",
      "          ಶೀ       0.67      1.00      0.80         4\n",
      "          ಶು       0.60      0.43      0.50         7\n",
      "          ಶೂ       0.40      1.00      0.57         2\n",
      "          ಶೃ       1.00      0.57      0.73         7\n",
      "          ಶೆ       0.60      1.00      0.75         3\n",
      "          ಶೈ       0.33      0.20      0.25         5\n",
      "          ಶೊ       0.17      0.25      0.20         4\n",
      "          ಶೌ       1.00      0.60      0.75         5\n",
      "           ಷ       1.00      0.43      0.60         7\n",
      "          ಷಂ       1.00      0.86      0.92         7\n",
      "          ಷಃ       1.00      0.33      0.50         3\n",
      "          ಷಾ       0.33      1.00      0.50         2\n",
      "          ಷಿ       0.50      0.25      0.33         8\n",
      "          ಷೀ       0.50      0.50      0.50         6\n",
      "          ಷು       0.67      0.80      0.73         5\n",
      "          ಷೂ       0.80      0.67      0.73         6\n",
      "          ಷೃ       1.00      0.86      0.92         7\n",
      "          ಷೆ       0.43      0.60      0.50         5\n",
      "          ಷೈ       0.78      0.78      0.78         9\n",
      "          ಷೊ       0.50      1.00      0.67         3\n",
      "          ಷೌ       1.00      0.50      0.67         6\n",
      "           ಸ       0.75      0.50      0.60         6\n",
      "          ಸಂ       1.00      0.75      0.86         4\n",
      "          ಸಃ       0.67      1.00      0.80         2\n",
      "          ಸಾ       0.67      1.00      0.80         2\n",
      "          ಸಿ       0.50      1.00      0.67         3\n",
      "          ಸೀ       0.67      0.67      0.67         3\n",
      "          ಸು       0.50      0.50      0.50         2\n",
      "          ಸೂ       0.80      0.67      0.73         6\n",
      "          ಸೃ       1.00      0.40      0.57         5\n",
      "          ಸೆ       1.00      0.70      0.82        10\n",
      "          ಸೈ       0.20      0.20      0.20         5\n",
      "          ಸೊ       0.78      0.88      0.82         8\n",
      "          ಸೌ       1.00      0.50      0.67         6\n",
      "           ಹ       0.40      1.00      0.57         2\n",
      "          ಹಂ       1.00      0.80      0.89         5\n",
      "          ಹಃ       0.83      0.83      0.83         6\n",
      "          ಹಾ       0.00      0.00      0.00         2\n",
      "          ಹಿ       1.00      0.57      0.73         7\n",
      "          ಹೀ       0.57      1.00      0.73         4\n",
      "          ಹು       0.50      0.67      0.57         6\n",
      "          ಹೂ       0.67      0.50      0.57         4\n",
      "          ಹೃ       1.00      0.70      0.82        10\n",
      "          ಹೆ       0.60      0.50      0.55         6\n",
      "          ಹೈ       0.83      0.83      0.83         6\n",
      "          ಹೊ       0.00      0.00      0.00         1\n",
      "          ಹೌ       0.75      0.60      0.67         5\n",
      "           ೠ       0.88      0.82      0.85        73\n",
      "\n",
      "    accuracy                           0.72      3462\n",
      "   macro avg       0.70      0.68      0.67      3462\n",
      "weighted avg       0.75      0.72      0.71      3462\n",
      "\n",
      "\n",
      "Model saved as 'tulu_ocr_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# MODEL TRAINING\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Dataset with Mapped Kannada Labels\n",
    "def load_dataset(root_folder, mapping_file, img_size=(64, 64)):\n",
    "    # Load the mapping from JSON\n",
    "    with open(mapping_file, 'r', encoding='utf-8') as f:\n",
    "        tulu_to_kannada = json.load(f)\n",
    "    \n",
    "    images, labels = [], []\n",
    "    \n",
    "    for folder in tqdm(tulu_to_kannada.keys(), desc=\"Loading Data\"):\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_name)\n",
    "                \n",
    "                try:\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, img_size)  # Resize for consistency\n",
    "                    images.append(img)\n",
    "                    labels.append(tulu_to_kannada[folder])  # Map Tulu folder to Kannada character\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Extract Features using HOG\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for img in tqdm(images, desc=\"Extracting Features\"):\n",
    "        hog_features = hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                           cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "        features.append(hog_features)\n",
    "    return np.array(features)\n",
    "\n",
    "# Train and Evaluate SVM Model\n",
    "def train_svm(features, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    svm = SVC(kernel='linear', C=1.0)\n",
    "    print(\"\\nTraining SVM Model...\")\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # Print Accuracy and Classification Report\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    return svm\n",
    "\n",
    "# Save Model to File\n",
    "def save_model(model, filename=\"tulu_ocr_model.pkl\"):\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"\\nModel saved as '{filename}'.\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = 'augmentation'  # Update dataset path\n",
    "    mapping_file = \"tulu_to_kannada_mapping.json\"\n",
    "\n",
    "    # Load images and labels\n",
    "    images, labels = load_dataset(root_folder, mapping_file)\n",
    "\n",
    "    # Extract HOG features\n",
    "    features = extract_features(images)\n",
    "\n",
    "    # Train SVM model\n",
    "    model = train_svm(features, labels)\n",
    "\n",
    "    # Save trained model\n",
    "    save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3bc7cc2-2c90-4db4-a6d2-7d89de4c700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Kannada Characetr: ಈ\n"
     ]
    }
   ],
   "source": [
    "# PREDICTION\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import hog\n",
    "import json\n",
    "\n",
    "# Load trained model\n",
    "model_filename = \"tulu_ocr_model.pkl\"\n",
    "svm_model = joblib.load(model_filename)\n",
    "\n",
    "# Load mapping file\n",
    "with open(\"tulu_to_kannada_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    character_mapping = json.load(f)\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(image_path, img_size=(64, 64)):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image at {image_path}\")\n",
    "        return None\n",
    "    img = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Extract HOG features\n",
    "    features = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                   cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "    \n",
    "    return np.array(features).reshape(1, -1)\n",
    "\n",
    "# Function to predict character\n",
    "def predict_character(image_path):\n",
    "    features = extract_features(image_path)\n",
    "    \n",
    "    if features is None:\n",
    "        print(\"Prediction aborted due to image loading error.\")\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    prediction_character = svm_model.predict(features)[0]  # Get predicted label\n",
    "    \n",
    "    print(f\"Predicted Kannada Characetr: {prediction_character}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_image_path = \"augmentation/character_5/left_slanted_3.png\"\n",
    "    predict_character(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a634e11-733e-43c3-aee3-7cfc15b20d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
