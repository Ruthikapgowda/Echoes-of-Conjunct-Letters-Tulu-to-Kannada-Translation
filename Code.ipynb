{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc51c8b4-8013-4f20-9431-f6232b8f6598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 112 sheets from folder: dataset/Lipi_1\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0001.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0002.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0003.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0004.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0005.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0006.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0007.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0008.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0009.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0010.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0011.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0012.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0013.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0014.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0015.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0016.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0017.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0018.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0019.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0020.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0021.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0022.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0023.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0024.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0025.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0026.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0027.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0028.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0029.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0030.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0031.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0032.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0033.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0034.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0035.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0036.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0037.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0038.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0039.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0040.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0041.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0042.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0043.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0044.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0045.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0046.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0047.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0048.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0049.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0050.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0051.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0052.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0053.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0054.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0055.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0056.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0057.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0058.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0059.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0060.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0061.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0062.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0063.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0064.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0065.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0066.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0067.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0068.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0069.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0070.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0071.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0072.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0073.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0074.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0075.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0076.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0077.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0078.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0079.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0080.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0081.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0082.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0083.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0084.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0085.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0086.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0087.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0088.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0089.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0090.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0091.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0092.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0093.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0094.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0095.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0096.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0097.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0098.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0099.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0100.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0101.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0102.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0103.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0104.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0105.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0106.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0107.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0108.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0109.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0110.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0111.jpg\n",
      "Processing sheet: dataset/Lipi_1\\DATASET_0112.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_1'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 0  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a0fb309-da78-4790-a4ff-33aab6ece9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_2\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 1_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 2_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 3_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 4_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 5_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 6_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 7_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 8_0001_page-0001.jpg\n",
      "Processing sheet: dataset/Lipi_2\\Lipi 9_0001_page-0001.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_2'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 16  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c47833a4-bb41-4d08-aa7a-44c2d721bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_3\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 1_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 2_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 3_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 4_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 5_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 6_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 7_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 8_0001_page-0002.jpg\n",
      "Processing sheet: dataset/Lipi_3\\Lipi 9_0001_page-0002.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_3'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 70  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c33ce50-4dfa-4f7a-a9a1-b043d1385cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_4\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 1_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 2_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 3_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 4_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 5_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 6_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 7_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 8_0001_page-0003.jpg\n",
      "Processing sheet: dataset/Lipi_4\\Lipi 9_0001_page-0003.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_4'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 124  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2eafabb0-3a10-41a9-acb1-270000ee7d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_5\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 1_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 2_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 3_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 4_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 5_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 6_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 7_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 8_0001_page-0004.jpg\n",
      "Processing sheet: dataset/Lipi_5\\Lipi 9_0001_page-0004.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_5'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 178  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cb0467d-49ba-4e2d-89a8-daaebcf340b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_6\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 1_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 2_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 3_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 4_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 5_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 6_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 7_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 8_0001_page-0005.jpg\n",
      "Processing sheet: dataset/Lipi_6\\Lipi 9_0001_page-0005.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_6'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 232  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2bee1bb-f2b1-4bc9-82d9-8740922db37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_7\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 1_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 2_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 3_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 4_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 5_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 6_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 7_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 8_0001_page-0006.jpg\n",
      "Processing sheet: dataset/Lipi_7\\Lipi 9_0001_page-0006.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_7'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 286  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe230904-9f9e-427c-b74e-eaa7f9e963bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_8\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 1_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 2_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 3_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 4_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 5_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 6_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 7_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 8_0001_page-0007.jpg\n",
      "Processing sheet: dataset/Lipi_8\\Lipi 9_0001_page-0007.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_8'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 340  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79780e6d-73d7-4aa1-a062-196626752c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_9\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 1_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 2_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 3_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 4_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 5_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 6_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 7_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 8_0001_page-0008.jpg\n",
      "Processing sheet: dataset/Lipi_9\\Lipi 9_0001_page-0008.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_9'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 394  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d4443ee-4898-4980-ac23-c7b17414974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 9 sheets from folder: dataset/Lipi_10\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 1_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 2_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 3_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 4_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 5_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 6_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 7_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 8_0001_page-0009.jpg\n",
      "Processing sheet: dataset/Lipi_10\\Lipi 9_0001_page-0009.jpg\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESSING\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_sheet(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Unable to load image at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to smooth the image and reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    # Adaptive thresholding to binarize the image\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 11, 8)\n",
    "    \n",
    "    # Invert the binary image to make characters black and background white\n",
    "    binary = cv2.bitwise_not(binary)\n",
    "    \n",
    "    return binary\n",
    "\n",
    "def extract_characters(binary_image, rows, cols, crop_percent=0.1):\n",
    "    if binary_image is None:\n",
    "        return []\n",
    "    \n",
    "    # Get the dimensions of the binary image\n",
    "    height, width = binary_image.shape\n",
    "    cell_width_pixels = width // cols\n",
    "    cell_height_pixels = height // rows\n",
    "    \n",
    "    crop_pixels_x = int(cell_width_pixels * crop_percent)\n",
    "    crop_pixels_y = int(cell_height_pixels * crop_percent)\n",
    "    \n",
    "    extracted_characters = []\n",
    "    \n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            x_start = col * cell_width_pixels + crop_pixels_x\n",
    "            y_start = row * cell_height_pixels + crop_pixels_y\n",
    "            x_end = (col + 1) * cell_width_pixels - crop_pixels_x\n",
    "            y_end = (row + 1) * cell_height_pixels - crop_pixels_y\n",
    "\n",
    "            # Ensure the coordinates are within bounds\n",
    "            x_start = max(x_start, 0)\n",
    "            y_start = max(y_start, 0)\n",
    "            x_end = min(x_end, width)\n",
    "            y_end = min(y_end, height)\n",
    "            \n",
    "            # Extract the cell and append it\n",
    "            cell = binary_image[y_start:y_end, x_start:x_end]\n",
    "            extracted_characters.append(cell)\n",
    "\n",
    "    return extracted_characters\n",
    "\n",
    "def save_characters(characters, output_folder, sheet_index, folder_start_number):\n",
    "    # Create root output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for index, character in enumerate(characters):\n",
    "        character_folder_number = folder_start_number + index + 1\n",
    "        character_folder = os.path.join(output_folder, f'character_{character_folder_number}')\n",
    "        if not os.path.exists(character_folder):\n",
    "            os.makedirs(character_folder)\n",
    "        \n",
    "        character_filename = f'{sheet_index + 1}.png'\n",
    "        character_path = os.path.join(character_folder, character_filename)\n",
    "        \n",
    "        # Save character image\n",
    "        cv2.imwrite(character_path, character)\n",
    "\n",
    "def process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number=1):\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"Input folder does not exist: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    # List all files in input folder\n",
    "    all_files = os.listdir(input_folder)\n",
    "    # print(f\"Files in input folder: {all_files}\")\n",
    "    \n",
    "    # Filter only .jpg files\n",
    "    sheet_files = sorted([f for f in all_files if f.lower().endswith('.jpg')])\n",
    "    if not sheet_files:\n",
    "        print(f\"No .jpg files found in the input folder: {input_folder}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Processing {len(sheet_files)} sheets from folder: {input_folder}\")\n",
    "\n",
    "    for sheet_index, sheet_file in enumerate(sheet_files):\n",
    "        sheet_path = os.path.join(input_folder, sheet_file)\n",
    "        print(f\"Processing sheet: {sheet_path}\")\n",
    "        \n",
    "        binarized_image = preprocess_sheet(sheet_path)\n",
    "        if binarized_image is None:\n",
    "            continue\n",
    "        \n",
    "        characters = extract_characters(binarized_image, rows, cols)\n",
    "        save_characters(characters, output_folder, sheet_index, folder_start_number)\n",
    "\n",
    "# Parameters\n",
    "input_folder = 'dataset/Lipi_10'  # Folder containing the images\n",
    "output_folder = 'preprocessing'  # Folder where the extracted characters will be saved\n",
    "rows = 6\n",
    "cols = 9\n",
    "folder_start_number = 448  # Starting number for character folders\n",
    "\n",
    "# Process all sheets\n",
    "process_all_sheets(input_folder, output_folder, rows, cols, folder_start_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c0fe0bd-c94f-4c44-b25e-c712c9da318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARIZATION\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_character_image(image, target_size=(50, 50)):\n",
    "    # Convert the image to grayscale if it isn't already\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    \n",
    "    # Apply adaptive thresholding (binarization) to get a binary image\n",
    "    binary_image = cv2.adaptiveThreshold(\n",
    "        blurred_image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, \n",
    "        cv2.THRESH_BINARY_INV, 11, 8\n",
    "    )\n",
    "    \n",
    "    # Resize to the target size (50x50)\n",
    "    resized_image = cv2.resize(binary_image, target_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image = resized_image / 255.0\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def process_dataset(input_folder, output_folder, target_size=(64, 64)):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Loop through each character folder\n",
    "    for character_folder in os.listdir(input_folder):\n",
    "        character_path = os.path.join(input_folder, character_folder)\n",
    "        \n",
    "        # Skip non-folder items\n",
    "        if not os.path.isdir(character_path):\n",
    "            continue\n",
    "\n",
    "        # Create the output directory for this character if it doesnt exist\n",
    "        character_output_folder = os.path.join(output_folder, character_folder)\n",
    "        if not os.path.exists(character_output_folder):\n",
    "            os.makedirs(character_output_folder)\n",
    "        \n",
    "        # Loop through images in this character's folder\n",
    "        for img_file in os.listdir(character_path):\n",
    "            img_path = os.path.join(character_path, img_file)\n",
    "            \n",
    "            # Load the image\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is None:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Preprocess the image with binarization\n",
    "            preprocessed_image = preprocess_character_image(image, target_size)\n",
    "            \n",
    "            # Save the preprocessed image\n",
    "            output_image_path = os.path.join(character_output_folder, img_file)\n",
    "            # Multiply by 255 to convert back to uint8 for saving\n",
    "            cv2.imwrite(output_image_path, (preprocessed_image * 255).astype(np.uint8))\n",
    "\n",
    "input_folder = 'preprocessing'   # Input folder with original character images\n",
    "output_folder = 'binarization'  # Output folder for preprocessed images\n",
    "target_size = (50, 50)  # Target size for each character image\n",
    "\n",
    "# Process the dataset\n",
    "process_dataset(input_folder, output_folder, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7e3cc06-0a6c-4e1b-a8d4-4492d52cf514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUGMENTATION\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "def augment_images_with_slant(input_folder, output_folder, target_size=(500, 500)):\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_subfolder = os.path.relpath(root, input_folder)\n",
    "\n",
    "            # Ensure the output subfolder exists\n",
    "            output_subfolder_path = os.path.join(output_folder, output_subfolder)\n",
    "            os.makedirs(output_subfolder_path, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                # Read the image\n",
    "                original_image = cv2.imread(input_path)\n",
    "\n",
    "                # Resize the image to the target size\n",
    "                resized_image = cv2.resize(original_image, target_size)\n",
    "\n",
    "                # Convert to grayscale and create a binary mask\n",
    "                gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "                _, binary_mask = cv2.threshold(gray_image, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                # Invert the mask to get background as 255 and characters as 0\n",
    "                binary_mask = cv2.bitwise_not(binary_mask)\n",
    "\n",
    "                # Define the slant angles for left and right\n",
    "                left_slant_angle = 10  # degrees\n",
    "                right_slant_angle = -10  # degrees\n",
    "\n",
    "                # Center of the image\n",
    "                center = (target_size[1] // 2, target_size[0] // 2)\n",
    "\n",
    "                # Compute the rotation matrices\n",
    "                left_rotation_matrix = cv2.getRotationMatrix2D(center, left_slant_angle, 1)\n",
    "                right_rotation_matrix = cv2.getRotationMatrix2D(center, right_slant_angle, 1)\n",
    "\n",
    "                # Apply the slant (rotation) transformations\n",
    "                left_slanted_image = cv2.warpAffine(resized_image, left_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "                right_slanted_image = cv2.warpAffine(resized_image, right_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "                # Apply the same transformations to the binary mask\n",
    "                left_slanted_mask = cv2.warpAffine(binary_mask, left_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "                right_slanted_mask = cv2.warpAffine(binary_mask, right_rotation_matrix, target_size, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "\n",
    "                # Mask out the white areas introduced by the rotation\n",
    "                left_slanted_image[left_slanted_mask == 255] = (0, 0, 0)\n",
    "                right_slanted_image[right_slanted_mask == 255] = (0, 0, 0)\n",
    "\n",
    "                # Save the original and slanted images\n",
    "                original_output_path = os.path.join(output_subfolder_path, f\"original_{file}\")\n",
    "                left_slanted_output_path = os.path.join(output_subfolder_path, f\"left_slanted_{file}\")\n",
    "                right_slanted_output_path = os.path.join(output_subfolder_path, f\"right_slanted_{file}\")\n",
    "\n",
    "                cv2.imwrite(original_output_path, resized_image)\n",
    "                cv2.imwrite(left_slanted_output_path, left_slanted_image)\n",
    "                cv2.imwrite(right_slanted_output_path, right_slanted_image)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {input_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your input folder and output folder for images\n",
    "    input_folder = 'binarization' # Change this to your input images folder path\n",
    "    output_folder = 'augmentation' # Change this to your output folder for augmented images\n",
    "\n",
    "    # Augment the images with left and right slants, and save the results\n",
    "    augment_images_with_slant(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9436888e-2481-44a6-901b-c8567abc325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping saved to 'tulu_to_kannada_mapping.json'.\n"
     ]
    }
   ],
   "source": [
    "# MAPPING\n",
    "import os\n",
    "import json\n",
    "\n",
    "def map_tulu_to_kannada(root_folder, output_file=\"tulu_to_kannada_mapping.json\"):\n",
    "    # Kannada characters list (ensure it matches the dataset size)\n",
    "    kannada_characters = [\n",
    "     \"_\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family \n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family \n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family \n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",\n",
    "    #  family\n",
    "    \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "    ]  \n",
    "\n",
    "    # Get all folder names in the root directory\n",
    "    folder_names = [f for f in os.listdir(root_folder) if os.path.isdir(os.path.join(root_folder, f))]\n",
    "\n",
    "    # Ensure correct ordering (sort numerically if folder names contain numbers)\n",
    "    folder_names_sorted = sorted(folder_names, key=lambda x: int(''.join(filter(str.isdigit, x))) if any(c.isdigit() for c in x) else x)\n",
    "\n",
    "    # Check if we have enough Kannada characters\n",
    "    if len(folder_names_sorted) > len(kannada_characters):\n",
    "        print(\"Error: Not enough Kannada characters available for mapping.\")\n",
    "        return\n",
    "\n",
    "    # Create a dictionary mapping each folder to a Kannada character\n",
    "    mapping = {folder: kannada_characters[idx] for idx, folder in enumerate(folder_names_sorted)}\n",
    "\n",
    "    # Save mapping to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(mapping, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Mapping saved to '{output_file}'.\")\n",
    "    return mapping\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = 'augmentation'  # Update with your dataset path\n",
    "    mapping = map_tulu_to_kannada(root_folder)\n",
    "    # print(mapping,) Print to verify mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d53f488f-b555-4a1d-a1f4-02459a7d77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|| 458/458 [03:36<00:00,  2.11it/s]\n",
      "Extracting Features: 100%|| 17310/17310 [00:30<00:00, 576.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training SVM Model...\n",
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.7158\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           _       0.81      0.84      0.83        76\n",
      "                  0.62      0.85      0.72        66\n",
      "                 0.71      0.66      0.68        80\n",
      "                 0.58      0.73      0.65        63\n",
      "                  0.83      0.90      0.86        58\n",
      "                  0.69      0.84      0.76        69\n",
      "                  0.73      0.81      0.77        73\n",
      "                  0.66      0.81      0.72        52\n",
      "                  0.78      0.82      0.80        71\n",
      "                  0.87      0.92      0.90        66\n",
      "                  0.81      0.85      0.83        66\n",
      "                  0.81      0.88      0.84        73\n",
      "                  0.84      0.85      0.85        62\n",
      "                  0.86      0.85      0.86        73\n",
      "                  0.79      0.77      0.78        62\n",
      "                  0.71      0.83      0.77         6\n",
      "                 1.00      1.00      1.00         2\n",
      "                 1.00      0.40      0.57         5\n",
      "                 0.83      1.00      0.91         5\n",
      "                 0.62      0.83      0.71         6\n",
      "                 1.00      0.70      0.82        10\n",
      "                 0.44      0.80      0.57         5\n",
      "                 0.80      0.57      0.67         7\n",
      "                 0.00      0.00      0.00         2\n",
      "                 0.56      1.00      0.71         5\n",
      "                 0.00      0.00      0.00         5\n",
      "                 0.75      0.75      0.75         4\n",
      "                 0.00      0.00      0.00         0\n",
      "                  0.80      0.80      0.80         5\n",
      "                 1.00      0.75      0.86         4\n",
      "                 0.90      0.90      0.90        10\n",
      "                 0.67      0.67      0.67         6\n",
      "                 0.43      0.60      0.50         5\n",
      "                 1.00      0.80      0.89         5\n",
      "                 0.00      0.00      0.00         5\n",
      "                 0.71      0.83      0.77         6\n",
      "                 1.00      0.60      0.75         5\n",
      "                 1.00      0.56      0.71         9\n",
      "                 0.50      0.40      0.44         5\n",
      "                 0.50      0.67      0.57         3\n",
      "                 1.00      0.50      0.67         6\n",
      "                  0.80      0.67      0.73         6\n",
      "                 1.00      1.00      1.00         9\n",
      "                 0.83      1.00      0.91         5\n",
      "                 0.43      0.60      0.50         5\n",
      "                 1.00      0.62      0.77         8\n",
      "                 0.67      0.80      0.73         5\n",
      "                 0.67      0.22      0.33         9\n",
      "                 0.67      0.57      0.62         7\n",
      "                 0.29      0.50      0.36         4\n",
      "                 0.43      0.60      0.50         5\n",
      "                 0.50      0.50      0.50         4\n",
      "                 0.57      0.67      0.62         6\n",
      "                 0.57      1.00      0.73         4\n",
      "                  0.80      0.57      0.67         7\n",
      "                 0.25      0.33      0.29         3\n",
      "                 1.00      0.44      0.62         9\n",
      "                 0.60      0.75      0.67         4\n",
      "                 0.40      0.29      0.33         7\n",
      "                 0.33      0.67      0.44         3\n",
      "                 1.00      0.80      0.89         5\n",
      "                 0.67      1.00      0.80         2\n",
      "                 0.67      1.00      0.80         4\n",
      "                 0.75      0.75      0.75         4\n",
      "                 0.62      1.00      0.77         5\n",
      "                 1.00      0.20      0.33         5\n",
      "                 0.40      1.00      0.57         2\n",
      "                  1.00      1.00      1.00         5\n",
      "                 0.83      0.83      0.83         6\n",
      "                 1.00      1.00      1.00         4\n",
      "                 0.62      0.83      0.71         6\n",
      "                 0.57      1.00      0.73         4\n",
      "                 0.60      0.43      0.50         7\n",
      "                 0.60      0.50      0.55         6\n",
      "                 0.75      0.67      0.71         9\n",
      "                 1.00      0.50      0.67         2\n",
      "                 0.33      0.14      0.20         7\n",
      "                 0.62      0.71      0.67         7\n",
      "                 1.00      0.67      0.80         6\n",
      "                 1.00      1.00      1.00         4\n",
      "                  0.60      1.00      0.75         6\n",
      "                 0.67      0.80      0.73         5\n",
      "                 0.57      0.80      0.67         5\n",
      "                 0.50      0.67      0.57         3\n",
      "                 0.50      0.75      0.60         4\n",
      "                 0.40      0.40      0.40         5\n",
      "                 0.38      0.50      0.43         6\n",
      "                 0.57      0.40      0.47        10\n",
      "                 0.80      0.67      0.73         6\n",
      "                 0.80      0.67      0.73         6\n",
      "                 0.43      0.75      0.55         4\n",
      "                 0.00      0.00      0.00         3\n",
      "                 0.20      0.17      0.18         6\n",
      "                  0.60      0.75      0.67         4\n",
      "                 0.67      0.67      0.67         3\n",
      "                 0.75      0.43      0.55         7\n",
      "                 0.71      0.83      0.77         6\n",
      "                 0.67      0.67      0.67         3\n",
      "                 1.00      0.38      0.55         8\n",
      "                 0.50      0.67      0.57         6\n",
      "                 1.00      0.43      0.60         7\n",
      "                 0.57      0.67      0.62         6\n",
      "                 0.43      0.43      0.43         7\n",
      "                 0.71      0.50      0.59        10\n",
      "                 1.00      0.33      0.50         9\n",
      "                 0.56      1.00      0.71         5\n",
      "                  0.25      0.25      0.25         4\n",
      "                 0.33      0.33      0.33         3\n",
      "                 0.57      0.80      0.67         5\n",
      "                 0.57      1.00      0.73         4\n",
      "                 0.86      1.00      0.92         6\n",
      "                 1.00      0.75      0.86         8\n",
      "                 0.00      0.00      0.00         2\n",
      "                 0.40      0.40      0.40         5\n",
      "                 0.40      0.40      0.40         5\n",
      "                 0.67      0.67      0.67         6\n",
      "                 0.56      0.83      0.67         6\n",
      "                 1.00      0.50      0.67         6\n",
      "                 0.60      1.00      0.75         3\n",
      "                  0.67      0.67      0.67         6\n",
      "                 1.00      0.62      0.77         8\n",
      "                 0.25      0.67      0.36         3\n",
      "                 0.44      1.00      0.62         4\n",
      "                 0.57      0.57      0.57         7\n",
      "                 0.67      0.40      0.50         5\n",
      "                 0.83      0.71      0.77         7\n",
      "                 0.50      1.00      0.67         3\n",
      "                 1.00      0.29      0.44         7\n",
      "                 0.88      0.88      0.88         8\n",
      "                 0.88      0.70      0.78        10\n",
      "                 0.80      1.00      0.89         4\n",
      "                 0.00      0.00      0.00         3\n",
      "                  0.00      0.00      0.00         2\n",
      "                 0.67      0.50      0.57         4\n",
      "                 0.67      0.67      0.67         3\n",
      "                 1.00      0.33      0.50         3\n",
      "                 0.60      0.60      0.60         5\n",
      "                 0.56      1.00      0.71         5\n",
      "                 0.86      0.60      0.71        10\n",
      "                 0.33      0.17      0.22         6\n",
      "                 0.50      0.75      0.60         4\n",
      "                 1.00      0.67      0.80         6\n",
      "                 0.42      0.83      0.56         6\n",
      "                 0.67      0.67      0.67         3\n",
      "                 1.00      0.86      0.92         7\n",
      "                  0.90      0.90      0.90        10\n",
      "                 0.75      0.60      0.67         5\n",
      "                 0.50      1.00      0.67         3\n",
      "                 1.00      0.50      0.67         6\n",
      "                 0.44      0.80      0.57         5\n",
      "                 0.75      0.38      0.50         8\n",
      "                 1.00      1.00      1.00         3\n",
      "                 0.80      1.00      0.89         4\n",
      "                 1.00      0.64      0.78        11\n",
      "                 1.00      1.00      1.00         2\n",
      "                 1.00      1.00      1.00         3\n",
      "                 0.50      1.00      0.67         2\n",
      "                 0.83      1.00      0.91         5\n",
      "                  0.77      0.77      0.77        13\n",
      "                 1.00      1.00      1.00         6\n",
      "                 0.60      1.00      0.75         6\n",
      "                 0.75      1.00      0.86         6\n",
      "                 0.86      0.75      0.80         8\n",
      "                 1.00      0.83      0.91         6\n",
      "                 0.43      0.60      0.50         5\n",
      "                 1.00      0.56      0.71         9\n",
      "                 0.90      0.82      0.86        11\n",
      "                 0.83      0.71      0.77         7\n",
      "                 0.78      1.00      0.88         7\n",
      "                 0.86      0.86      0.86         7\n",
      "                 0.88      1.00      0.93         7\n",
      "                  0.33      0.17      0.22         6\n",
      "                 0.60      0.43      0.50         7\n",
      "                 0.75      0.75      0.75         4\n",
      "                 0.67      0.67      0.67         3\n",
      "                 0.75      0.38      0.50         8\n",
      "                 0.57      0.80      0.67         5\n",
      "                0.50      0.83      0.62         6\n",
      "                 0.80      0.67      0.73         6\n",
      "                 0.20      0.14      0.17         7\n",
      "                 0.50      0.43      0.46         7\n",
      "                 0.00      0.00      0.00         4\n",
      "                 0.60      0.23      0.33        13\n",
      "                 0.29      0.67      0.40         3\n",
      "                  0.62      0.83      0.71         6\n",
      "                 1.00      0.50      0.67         6\n",
      "                 0.57      0.67      0.62         6\n",
      "                 0.60      0.38      0.46         8\n",
      "                 0.75      0.50      0.60         6\n",
      "                 0.57      0.67      0.62         6\n",
      "                 0.75      1.00      0.86         3\n",
      "                 0.33      0.75      0.46         4\n",
      "                 0.50      0.43      0.46         7\n",
      "                 0.60      0.50      0.55         6\n",
      "                 0.71      0.83      0.77         6\n",
      "                 0.57      0.57      0.57         7\n",
      "                 0.75      0.86      0.80         7\n",
      "                  0.71      0.83      0.77         6\n",
      "                 0.50      0.67      0.57         3\n",
      "                 0.67      0.67      0.67         3\n",
      "                 1.00      0.25      0.40         4\n",
      "                 0.57      1.00      0.73         4\n",
      "                 0.80      0.67      0.73         6\n",
      "                 0.67      0.80      0.73         5\n",
      "                 0.50      0.20      0.29         5\n",
      "                 1.00      0.40      0.57         5\n",
      "                 0.80      0.57      0.67         7\n",
      "                 0.50      0.60      0.55         5\n",
      "                 0.71      1.00      0.83         5\n",
      "                 0.75      1.00      0.86         3\n",
      "                  1.00      0.67      0.80         3\n",
      "                 1.00      0.75      0.86         4\n",
      "                 1.00      1.00      1.00         4\n",
      "                 1.00      1.00      1.00         5\n",
      "                 1.00      1.00      1.00         5\n",
      "                 1.00      0.75      0.86         4\n",
      "                 0.60      0.60      0.60         5\n",
      "                 1.00      0.86      0.92         7\n",
      "                 0.33      0.33      0.33         3\n",
      "                 0.67      1.00      0.80         4\n",
      "                 0.88      0.88      0.88         8\n",
      "                 1.00      0.75      0.86         4\n",
      "                 1.00      0.86      0.92         7\n",
      "                  1.00      0.71      0.83         7\n",
      "                 0.67      1.00      0.80         2\n",
      "                 1.00      0.80      0.89         5\n",
      "                 0.33      0.50      0.40         4\n",
      "                 0.86      0.75      0.80         8\n",
      "                 0.80      0.44      0.57         9\n",
      "                 0.83      1.00      0.91         5\n",
      "                 1.00      1.00      1.00         4\n",
      "                 0.75      1.00      0.86         3\n",
      "                 0.50      0.57      0.53         7\n",
      "                 1.00      0.40      0.57         5\n",
      "                 0.25      0.25      0.25         4\n",
      "                 0.29      0.67      0.40         3\n",
      "                  0.25      0.20      0.22         5\n",
      "                 0.80      0.80      0.80         5\n",
      "                 1.00      0.57      0.73         7\n",
      "                 0.75      0.60      0.67         5\n",
      "                 0.71      1.00      0.83         5\n",
      "                 1.00      0.86      0.92         7\n",
      "                 1.00      0.50      0.67         6\n",
      "                 0.71      1.00      0.83         5\n",
      "                 1.00      0.80      0.89         5\n",
      "                 1.00      1.00      1.00         3\n",
      "                 0.57      0.80      0.67         5\n",
      "                 0.83      0.42      0.56        12\n",
      "                 0.67      0.57      0.62         7\n",
      "                  1.00      1.00      1.00         3\n",
      "                 1.00      0.88      0.93         8\n",
      "                 1.00      0.67      0.80         6\n",
      "                 0.83      0.83      0.83         6\n",
      "                 0.00      0.00      0.00         2\n",
      "                 0.62      1.00      0.77         5\n",
      "                 1.00      0.71      0.83         7\n",
      "                 0.38      0.75      0.50         4\n",
      "                 1.00      0.33      0.50         6\n",
      "                 0.60      0.75      0.67         4\n",
      "                 0.45      1.00      0.62         5\n",
      "                 1.00      0.67      0.80         9\n",
      "                 0.50      1.00      0.67         2\n",
      "                  0.25      0.33      0.29         3\n",
      "                 1.00      0.86      0.92         7\n",
      "                 1.00      1.00      1.00         5\n",
      "                 0.75      0.75      0.75         4\n",
      "                 1.00      1.00      1.00         6\n",
      "                 0.80      1.00      0.89         4\n",
      "                 0.75      0.75      0.75         4\n",
      "                 0.50      1.00      0.67         1\n",
      "                 0.83      0.83      0.83         6\n",
      "                 0.00      0.00      0.00         0\n",
      "                 0.56      0.71      0.62         7\n",
      "                 0.50      0.20      0.29         5\n",
      "                 1.00      0.80      0.89         5\n",
      "                  1.00      0.80      0.89         5\n",
      "                 1.00      1.00      1.00         5\n",
      "                 0.00      0.00      0.00         2\n",
      "                 0.67      0.50      0.57         4\n",
      "                 0.40      0.67      0.50         3\n",
      "                 0.83      0.56      0.67         9\n",
      "                 0.57      1.00      0.73         4\n",
      "                 1.00      0.62      0.77         8\n",
      "                 1.00      0.50      0.67         6\n",
      "                 1.00      0.71      0.83         7\n",
      "                 0.50      0.40      0.44         5\n",
      "                 0.86      0.86      0.86         7\n",
      "                 0.50      0.50      0.50         4\n",
      "                  0.67      0.50      0.57         4\n",
      "                 0.83      1.00      0.91         5\n",
      "                 1.00      0.60      0.75         5\n",
      "                 0.83      0.83      0.83         6\n",
      "                 0.71      0.83      0.77         6\n",
      "                 0.50      0.11      0.18         9\n",
      "                 0.60      0.60      0.60         5\n",
      "                 0.33      0.50      0.40         4\n",
      "                 0.40      0.50      0.44         4\n",
      "                 0.75      0.75      0.75         4\n",
      "                 0.67      0.57      0.62         7\n",
      "                 0.43      0.60      0.50         5\n",
      "                 0.33      0.25      0.29         4\n",
      "                  0.67      0.80      0.73         5\n",
      "                 1.00      1.00      1.00         3\n",
      "                 0.60      0.75      0.67         4\n",
      "                 0.00      0.00      0.00         4\n",
      "                 0.50      0.60      0.55         5\n",
      "                 0.75      0.43      0.55         7\n",
      "                 0.57      0.67      0.62         6\n",
      "                 0.50      0.29      0.36         7\n",
      "                 0.67      0.50      0.57         4\n",
      "                 0.50      0.50      0.50         2\n",
      "                 0.83      0.71      0.77         7\n",
      "                 0.75      0.75      0.75         4\n",
      "                 1.00      0.50      0.67         8\n",
      "                  0.50      0.80      0.62         5\n",
      "                 1.00      1.00      1.00         5\n",
      "                 1.00      0.50      0.67         6\n",
      "                 0.18      0.67      0.29         3\n",
      "                 0.67      1.00      0.80         6\n",
      "                 1.00      0.50      0.67         8\n",
      "                 0.50      0.25      0.33         4\n",
      "                 0.50      0.50      0.50         6\n",
      "                 1.00      1.00      1.00         3\n",
      "                 0.75      0.60      0.67         5\n",
      "                 1.00      0.71      0.83         7\n",
      "                 1.00      0.33      0.50         6\n",
      "                 0.50      1.00      0.67         4\n",
      "                  0.50      0.80      0.62         5\n",
      "                 0.86      1.00      0.92         6\n",
      "                 1.00      1.00      1.00         1\n",
      "                 0.80      0.67      0.73         6\n",
      "                 1.00      0.60      0.75         5\n",
      "                 1.00      1.00      1.00         8\n",
      "                 0.67      0.57      0.62         7\n",
      "                 0.50      0.60      0.55         5\n",
      "                 0.60      0.50      0.55         6\n",
      "                 1.00      0.83      0.91         6\n",
      "                 0.75      1.00      0.86         3\n",
      "                 0.50      1.00      0.67         3\n",
      "                 1.00      1.00      1.00         5\n",
      "                  0.67      0.25      0.36         8\n",
      "                 0.83      0.71      0.77         7\n",
      "                 1.00      0.86      0.92         7\n",
      "                 0.50      0.25      0.33         4\n",
      "                 0.67      0.67      0.67         6\n",
      "                 0.83      0.71      0.77         7\n",
      "                 0.57      0.57      0.57         7\n",
      "                 0.20      0.25      0.22         4\n",
      "                 0.75      0.60      0.67         5\n",
      "                 0.67      0.44      0.53         9\n",
      "                 0.43      1.00      0.60         3\n",
      "                 0.40      0.40      0.40         5\n",
      "                 0.60      0.50      0.55         6\n",
      "                  0.71      1.00      0.83         5\n",
      "                 1.00      0.88      0.93         8\n",
      "                 1.00      1.00      1.00         6\n",
      "                 1.00      0.80      0.89         5\n",
      "                 0.86      1.00      0.92         6\n",
      "                 1.00      1.00      1.00         3\n",
      "                 1.00      1.00      1.00         3\n",
      "                 1.00      1.00      1.00         5\n",
      "                 1.00      1.00      1.00         5\n",
      "                 1.00      0.36      0.53        11\n",
      "                 0.60      1.00      0.75         6\n",
      "                 0.80      0.80      0.80         5\n",
      "                 0.60      0.60      0.60         5\n",
      "                  1.00      0.50      0.67         8\n",
      "                 0.86      0.86      0.86         7\n",
      "                 0.60      0.75      0.67         8\n",
      "                 0.67      1.00      0.80         4\n",
      "                 0.67      1.00      0.80         2\n",
      "                 1.00      1.00      1.00         5\n",
      "                 0.67      1.00      0.80         4\n",
      "                 1.00      0.40      0.57         5\n",
      "                 1.00      0.60      0.75         5\n",
      "                 0.57      0.50      0.53         8\n",
      "                 1.00      1.00      1.00         7\n",
      "                 0.67      1.00      0.80         4\n",
      "                 0.60      1.00      0.75         3\n",
      "                  0.25      0.17      0.20         6\n",
      "                 1.00      0.50      0.67        10\n",
      "                 0.75      0.75      0.75         4\n",
      "                 0.89      0.80      0.84        10\n",
      "                 0.80      1.00      0.89         4\n",
      "                 1.00      0.67      0.80         3\n",
      "                 0.75      0.50      0.60         6\n",
      "                 0.62      0.83      0.71         6\n",
      "                 0.80      1.00      0.89         4\n",
      "                 0.80      0.80      0.80         5\n",
      "                 1.00      0.60      0.75         5\n",
      "                 0.60      1.00      0.75         3\n",
      "                 1.00      0.78      0.88         9\n",
      "                  0.91      1.00      0.95        10\n",
      "                 0.75      1.00      0.86         3\n",
      "                 0.83      0.83      0.83         6\n",
      "                 0.20      0.50      0.29         4\n",
      "                 0.71      0.56      0.62         9\n",
      "                 1.00      0.33      0.50         6\n",
      "                 0.33      1.00      0.50         1\n",
      "                 1.00      0.71      0.83         7\n",
      "                 1.00      0.75      0.86         8\n",
      "                 0.67      0.50      0.57         8\n",
      "                 0.67      0.29      0.40         7\n",
      "                 0.67      0.80      0.73         5\n",
      "                 0.75      0.75      0.75         4\n",
      "                  0.80      1.00      0.89         4\n",
      "                 1.00      1.00      1.00         6\n",
      "                 0.67      1.00      0.80         2\n",
      "                 0.50      1.00      0.67         1\n",
      "                 1.00      0.57      0.73         7\n",
      "                 0.67      1.00      0.80         4\n",
      "                 0.60      0.43      0.50         7\n",
      "                 0.40      1.00      0.57         2\n",
      "                 1.00      0.57      0.73         7\n",
      "                 0.60      1.00      0.75         3\n",
      "                 0.33      0.20      0.25         5\n",
      "                 0.17      0.25      0.20         4\n",
      "                 1.00      0.60      0.75         5\n",
      "                  1.00      0.43      0.60         7\n",
      "                 1.00      0.86      0.92         7\n",
      "                 1.00      0.33      0.50         3\n",
      "                 0.33      1.00      0.50         2\n",
      "                 0.50      0.25      0.33         8\n",
      "                 0.50      0.50      0.50         6\n",
      "                 0.67      0.80      0.73         5\n",
      "                 0.80      0.67      0.73         6\n",
      "                 1.00      0.86      0.92         7\n",
      "                 0.43      0.60      0.50         5\n",
      "                 0.78      0.78      0.78         9\n",
      "                 0.50      1.00      0.67         3\n",
      "                 1.00      0.50      0.67         6\n",
      "                  0.75      0.50      0.60         6\n",
      "                 1.00      0.75      0.86         4\n",
      "                 0.67      1.00      0.80         2\n",
      "                 0.67      1.00      0.80         2\n",
      "                 0.50      1.00      0.67         3\n",
      "                 0.67      0.67      0.67         3\n",
      "                 0.50      0.50      0.50         2\n",
      "                 0.80      0.67      0.73         6\n",
      "                 1.00      0.40      0.57         5\n",
      "                 1.00      0.70      0.82        10\n",
      "                 0.20      0.20      0.20         5\n",
      "                 0.78      0.88      0.82         8\n",
      "                 1.00      0.50      0.67         6\n",
      "                  0.40      1.00      0.57         2\n",
      "                 1.00      0.80      0.89         5\n",
      "                 0.83      0.83      0.83         6\n",
      "                 0.00      0.00      0.00         2\n",
      "                 1.00      0.57      0.73         7\n",
      "                 0.57      1.00      0.73         4\n",
      "                 0.50      0.67      0.57         6\n",
      "                 0.67      0.50      0.57         4\n",
      "                 1.00      0.70      0.82        10\n",
      "                 0.60      0.50      0.55         6\n",
      "                 0.83      0.83      0.83         6\n",
      "                 0.00      0.00      0.00         1\n",
      "                 0.75      0.60      0.67         5\n",
      "                  0.88      0.82      0.85        73\n",
      "\n",
      "    accuracy                           0.72      3462\n",
      "   macro avg       0.70      0.68      0.67      3462\n",
      "weighted avg       0.75      0.72      0.71      3462\n",
      "\n",
      "\n",
      "Model saved as 'tulu_ocr_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# MODEL TRAINING\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load Dataset with Mapped Kannada Labels\n",
    "def load_dataset(root_folder, mapping_file, img_size=(64, 64)):\n",
    "    # Load the mapping from JSON\n",
    "    with open(mapping_file, 'r', encoding='utf-8') as f:\n",
    "        tulu_to_kannada = json.load(f)\n",
    "    \n",
    "    images, labels = [], []\n",
    "    \n",
    "    for folder in tqdm(tulu_to_kannada.keys(), desc=\"Loading Data\"):\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_name)\n",
    "                \n",
    "                try:\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, img_size)  # Resize for consistency\n",
    "                    images.append(img)\n",
    "                    labels.append(tulu_to_kannada[folder])  # Map Tulu folder to Kannada character\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Extract Features using HOG\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for img in tqdm(images, desc=\"Extracting Features\"):\n",
    "        hog_features = hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                           cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "        features.append(hog_features)\n",
    "    return np.array(features)\n",
    "\n",
    "# Train and Evaluate SVM Model\n",
    "def train_svm(features, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    svm = SVC(kernel='linear', C=1.0)\n",
    "    print(\"\\nTraining SVM Model...\")\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = svm.predict(X_test)\n",
    "\n",
    "    # Print Accuracy and Classification Report\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    return svm\n",
    "\n",
    "# Save Model to File\n",
    "def save_model(model, filename=\"tulu_ocr_model.pkl\"):\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"\\nModel saved as '{filename}'.\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    root_folder = 'augmentation'  # Update dataset path\n",
    "    mapping_file = \"tulu_to_kannada_mapping.json\"\n",
    "\n",
    "    # Load images and labels\n",
    "    images, labels = load_dataset(root_folder, mapping_file)\n",
    "\n",
    "    # Extract HOG features\n",
    "    features = extract_features(images)\n",
    "\n",
    "    # Train SVM model\n",
    "    model = train_svm(features, labels)\n",
    "\n",
    "    # Save trained model\n",
    "    save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3bc7cc2-2c90-4db4-a6d2-7d89de4c700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Kannada Characetr: \n"
     ]
    }
   ],
   "source": [
    "# PREDICTION\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import hog\n",
    "import json\n",
    "\n",
    "# Load trained model\n",
    "model_filename = \"tulu_ocr_model.pkl\"\n",
    "svm_model = joblib.load(model_filename)\n",
    "\n",
    "# Load mapping file\n",
    "with open(\"tulu_to_kannada_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    character_mapping = json.load(f)\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(image_path, img_size=(64, 64)):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image at {image_path}\")\n",
    "        return None\n",
    "    img = cv2.resize(img, img_size)\n",
    "    \n",
    "    # Extract HOG features\n",
    "    features = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                   cells_per_block=(2, 2), block_norm='L2-Hys')\n",
    "    \n",
    "    return np.array(features).reshape(1, -1)\n",
    "\n",
    "# Function to predict character\n",
    "def predict_character(image_path):\n",
    "    features = extract_features(image_path)\n",
    "    \n",
    "    if features is None:\n",
    "        print(\"Prediction aborted due to image loading error.\")\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    prediction_character = svm_model.predict(features)[0]  # Get predicted label\n",
    "    \n",
    "    print(f\"Predicted Kannada Characetr: {prediction_character}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_image_path = \"augmentation/character_5/left_slanted_3.png\"\n",
    "    predict_character(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a634e11-733e-43c3-aee3-7cfc15b20d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
